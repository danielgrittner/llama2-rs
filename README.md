# llama2-rs

Goal: run LLaMA2 locally on a CPU (and be faster than llama2.c from Andrej Karpathy :) )

TODOs:

- [X] Implement loading of the model
- [ ] Implement forward pass
- [ ] Implement generation
- [ ] Implement benchmarking
- [ ] Optimize performance (parallelization, SIMD, etc.)
- [ ] Support prompting and tokenization

